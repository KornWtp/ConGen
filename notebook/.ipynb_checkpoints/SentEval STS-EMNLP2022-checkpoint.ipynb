{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import senteval\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentEval prepare and batcher\n",
    "def prepare(params, samples):\n",
    "    return\n",
    "\n",
    "def batcher(params, batch):\n",
    "    batch = [' '.join(sent) if sent != [] else '.' for sent in batch]\n",
    "    embeddings = params['encoder'](batch)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name output/Simcse_original_BERT_original_cls_MLPTrue. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:output/Simcse_original_BERT_original_cls_MLPTrue\n",
      "Total Trainable Params: 109,482,240\n",
      "Model:output/Simcse_original_nreimers_albert_small_v2_cls_MLPTrue\n",
      "Total Trainable Params: 12,274,176\n",
      "Model:output/Simcse_original_nreimers_BERT_Mini_L_4_H_256_A_4_cls_MLPTrue\n",
      "Total Trainable Params: 11,236,352\n",
      "Model:output/Simcse_original_nreimers_MiniLM_L6_H384_uncased_cls_MLPTrue\n",
      "Total Trainable Params: 22,861,056\n",
      "Model:output/Simcse_original_bert_large_uncased_cls_MLPTrue\n",
      "Total Trainable Params: 336,191,488\n",
      "Model:output/Simcse_original_distilbert_base_cased_cls_MLPTrue\n",
      "Total Trainable Params: 65,781,504\n",
      "Model:output/Simcse_original_nreimers_TinyBERT_L_4_H_312_v2_cls_MLPTrue\n",
      "Total Trainable Params: 14,447,904\n",
      "Model:output/Simcse_original_roberta_base_cls_MLPTrue\n",
      "Total Trainable Params: 125,236,224\n",
      "Model:output/Simcse_original_nreimers_BERT_Tiny_L_2_H_128_A_2_cls_MLPTrue\n",
      "Total Trainable Params: 4,402,432\n",
      "Model:output/Simcse_original_roberta_large_cls_MLPTrue\n",
      "Total Trainable Params: 356,409,344\n",
      "Model:output/Simcse_original_google_mobilebert_uncased_cls_MLPTrue\n",
      "Total Trainable Params: 24,844,544\n",
      "Model:output/Simcse_original_nreimers_BERT_Small_L_4_H_512_A_8_cls_MLPTrue\n",
      "Total Trainable Params: 29,026,304\n",
      "Model:output/Simcse_original_bert_base_uncased_cls_MLPTrue\n",
      "Total Trainable Params: 110,072,832\n",
      "Model:output/Simcse_original_nreimers_TinyBERT_L_6_H_768_v2_cls_MLPTrue\n",
      "Total Trainable Params: 67,545,600\n",
      "Model:output/Simcse_original_nreimers_MiniLM_L3_H384_uncased_cls_MLPTrue\n",
      "Total Trainable Params: 17,537,664\n",
      "Model:output/Simcse_original_microsoft_MiniLM_L12_H384_uncased_cls_MLPTrue\n",
      "Total Trainable Params: 33,507,840\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: \n",
    "            continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "#     print(table)\n",
    "    print(f\"Total Trainable Params: {total_params:,}\")\n",
    "    return total_params\n",
    "\n",
    "pool_mode='cls'\n",
    "mlp_mode=True\n",
    "model_list = glob(f'output/Simcse_original_*{pool_mode}*MLP{mlp_mode}*')\n",
    "for m in model_list[:]:\n",
    "    print(f\"Model:{m}\")\n",
    "    sim_cse = SentenceTransformer(m)\n",
    "    \n",
    "    count_parameters(sim_cse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:microsoft/Multilingual-MiniLM-L12-H384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/microsoft_Multilingual-MiniLM-L12-H384. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Params: 117,653,760\n",
      "Model:distilbert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-multilingual-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/distilbert-base-multilingual-cased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Params: 134,734,080\n",
      "Model:bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/bert-base-multilingual-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Params: 177,853,440\n",
      "Model:xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/xlm-roberta-base. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Params: 278,043,648\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: \n",
    "            continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "#     print(table)\n",
    "    print(f\"Total Trainable Params: {total_params:,}\")\n",
    "    return total_params\n",
    "\n",
    "pool_mode='cls'\n",
    "mlp_mode=True\n",
    "model_list = ['microsoft/Multilingual-MiniLM-L12-H384','distilbert-base-multilingual-cased','bert-base-multilingual-cased','xlm-roberta-base']\n",
    "for m in model_list[:]:\n",
    "    print(f\"Model:{m}\")\n",
    "    sim_cse = SentenceTransformer(m)\n",
    "    \n",
    "    count_parameters(sim_cse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConGen \n",
    "\n",
    "we show only 1 random seed, if you want 3 random seeds, please see our github link (attached in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_score(avg_score,file_name, score_list, name_list):\n",
    "    for idx,_ in enumerate(score_list):\n",
    "        if avg_score > score_list[idx]:\n",
    "            score_list.insert(idx,avg_score)\n",
    "            name_list.insert(idx,file_name)\n",
    "            break\n",
    "    return score_list, name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:congen_models/Asym_BERT-Small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/m_sentence_embedding/SimCSE/SentEval/senteval/sts.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  sent1 = np.array([s.split() for s in sent1])[not_empty_idx]\n",
      "/workspace/m_sentence_embedding/SimCSE/SentEval/senteval/sts.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  sent2 = np.array([s.split() for s in sent2])[not_empty_idx]\n",
      " 10%|█         | 1/10 [01:02<09:24, 62.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg:78.83\n",
      "**************************************************\n",
      "Model:congen_models/Asym_MiniLM-L3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:43<07:27, 55.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg:78.36\n",
      "**************************************************\n",
      "Model:congen_models/Asym_TinyBERT_L-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [02:37<06:28, 55.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg:78.64\n",
      "**************************************************\n",
      "Model:congen_models/Asym_MiniLM-L12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [03:49<06:02, 60.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg:79.7\n",
      "**************************************************\n",
      "Model:congen_models/Asym_TinyBERT_L-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [04:55<05:11, 62.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg:79.77\n",
      "**************************************************\n",
      "Model:congen_models/Asym_roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [06:42<05:02, 75.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg:79.89\n",
      "**************************************************\n",
      "Model:congen_models/Asym_BERT-Mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [07:34<03:25, 68.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg:78.17\n",
      "**************************************************\n",
      "Model:congen_models/Asym_bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [09:21<02:40, 80.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg:80.09\n",
      "**************************************************\n",
      "Model:congen_models/Asym_BERT-Tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [09:59<01:07, 67.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg:77.12\n",
      "**************************************************\n",
      "Model:congen_models/Asym_MiniLM-L6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [10:47<00:00, 64.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg:79.2\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_list = glob(f'congen_models/Asym*')\n",
    "\n",
    "all_scores = [0] * len(model_list)\n",
    "name_files = []\n",
    "for model in tqdm(model_list[:]):\n",
    "    print(f\"Model:{model}\")\n",
    "    sim_cse = SentenceTransformer(model)\n",
    "\n",
    "    params = {'task_path': 'SentEval/data/', 'usepytorch': True, 'kfold': 10}\n",
    "    params['encoder'] = sim_cse.encode\n",
    "    se = senteval.engine.SE(params, batcher, prepare)\n",
    "\n",
    "    transfer_tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n",
    "    results = se.eval(transfer_tasks)\n",
    "\n",
    "    spearman_val = 0\n",
    "    text = []\n",
    "    for key in results.keys():\n",
    "        text.append(key)\n",
    "        if key not in ['STSBenchmark','SICKRelatedness']:\n",
    "            result_temp = results[key]['all']['spearman']['all']\n",
    "            spearman_val+=result_temp\n",
    "            text.append(f\"Spearman:{result_temp*100:.2f}\")\n",
    "        else:\n",
    "            result_temp = results[key]['test']['spearman'].correlation\n",
    "            spearman_val+=result_temp\n",
    "            text.append(f\"Spearman:{result_temp*100:.2f}\")\n",
    "    avg_score = round(spearman_val/len(results.keys())*100,2)\n",
    "    all_scores,name_files = sorting_score(avg_score,model,all_scores,name_files)\n",
    "    print(f\"Avg:{avg_score}\")\n",
    "    print(f\"*\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Mono T = SimCSE(Wiki) S = small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:output/l2_mono/L2-no-aug-T-princeton-nlp-unsup-simcse-roberta-large-S-bert-base-uncased\n",
      "STS12\n",
      "Spearman:74.07\n",
      "STS13\n",
      "Spearman:85.69\n",
      "STS14\n",
      "Spearman:77.37\n",
      "STS15\n",
      "Spearman:85.37\n",
      "STS16\n",
      "Spearman:80.38\n",
      "STSBenchmark\n",
      "Spearman:81.80\n",
      "SICKRelatedness\n",
      "Spearman:69.59\n",
      "Avg:79.18\n",
      "**************************************************\n",
      "\n",
      "Model:output/l2_mono/L2-no-aug-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-MiniLM-L3-H384-uncased\n",
      "STS12\n",
      "Spearman:70.68\n",
      "STS13\n",
      "Spearman:82.39\n",
      "STS14\n",
      "Spearman:74.80\n",
      "STS15\n",
      "Spearman:82.70\n",
      "STS16\n",
      "Spearman:76.91\n",
      "STSBenchmark\n",
      "Spearman:74.71\n",
      "SICKRelatedness\n",
      "Spearman:65.24\n",
      "Avg:75.35\n",
      "**************************************************\n",
      "\n",
      "Model:output/l2_mono/L2-no-aug-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-BERT-Tiny_L-2_H-128_A-2\n",
      "STS12\n",
      "Spearman:68.74\n",
      "STS13\n",
      "Spearman:78.81\n",
      "STS14\n",
      "Spearman:71.12\n",
      "STS15\n",
      "Spearman:78.80\n",
      "STS16\n",
      "Spearman:72.00\n",
      "STSBenchmark\n",
      "Spearman:67.92\n",
      "SICKRelatedness\n",
      "Spearman:61.55\n",
      "Avg:71.28\n",
      "**************************************************\n",
      "\n",
      "Model:output/l2_mono/L2-no-aug-T-princeton-nlp-unsup-simcse-roberta-large-S-roberta-base\n",
      "STS12\n",
      "Spearman:72.62\n",
      "STS13\n",
      "Spearman:84.92\n",
      "STS14\n",
      "Spearman:76.05\n",
      "STS15\n",
      "Spearman:84.59\n",
      "STS16\n",
      "Spearman:80.78\n",
      "STSBenchmark\n",
      "Spearman:81.73\n",
      "SICKRelatedness\n",
      "Spearman:70.17\n",
      "Avg:78.69\n",
      "**************************************************\n",
      "\n",
      "Model:output/l2_mono/L2-no-aug-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-BERT-Small-L-4_H-512_A-8\n",
      "STS12\n",
      "Spearman:71.56\n",
      "STS13\n",
      "Spearman:83.82\n",
      "STS14\n",
      "Spearman:75.88\n",
      "STS15\n",
      "Spearman:83.61\n",
      "STS16\n",
      "Spearman:78.23\n",
      "STSBenchmark\n",
      "Spearman:77.80\n",
      "SICKRelatedness\n",
      "Spearman:66.83\n",
      "Avg:76.82\n",
      "**************************************************\n",
      "\n",
      "Model:output/l2_mono/L2-no-aug-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-MiniLM-L6-H384-uncased\n",
      "STS12\n",
      "Spearman:71.63\n",
      "STS13\n",
      "Spearman:84.32\n",
      "STS14\n",
      "Spearman:76.05\n",
      "STS15\n",
      "Spearman:83.48\n",
      "STS16\n",
      "Spearman:77.90\n",
      "STSBenchmark\n",
      "Spearman:77.47\n",
      "SICKRelatedness\n",
      "Spearman:66.97\n",
      "Avg:76.83\n",
      "**************************************************\n",
      "\n",
      "Model:output/l2_mono/L2-no-aug-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-BERT-Mini_L-4_H-256_A-4\n",
      "STS12\n",
      "Spearman:70.84\n",
      "STS13\n",
      "Spearman:82.13\n",
      "STS14\n",
      "Spearman:74.12\n",
      "STS15\n",
      "Spearman:82.03\n",
      "STS16\n",
      "Spearman:75.59\n",
      "STSBenchmark\n",
      "Spearman:74.28\n",
      "SICKRelatedness\n",
      "Spearman:64.59\n",
      "Avg:74.80\n",
      "**************************************************\n",
      "\n",
      "Model:output/l2_mono/L2-no-aug-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-TinyBERT_L-6_H-768_v2\n",
      "STS12\n",
      "Spearman:73.67\n",
      "STS13\n",
      "Spearman:85.17\n",
      "STS14\n",
      "Spearman:77.06\n",
      "STS15\n",
      "Spearman:85.13\n",
      "STS16\n",
      "Spearman:80.51\n",
      "STSBenchmark\n",
      "Spearman:81.49\n",
      "SICKRelatedness\n",
      "Spearman:69.59\n",
      "Avg:78.95\n",
      "**************************************************\n",
      "\n",
      "Model:output/l2_mono/L2-no-aug-T-princeton-nlp-unsup-simcse-roberta-large-S-microsoft-MiniLM-L12-H384-uncased\n",
      "STS12\n",
      "Spearman:73.10\n",
      "STS13\n",
      "Spearman:85.08\n",
      "STS14\n",
      "Spearman:77.03\n",
      "STS15\n",
      "Spearman:84.57\n",
      "STS16\n",
      "Spearman:79.36\n",
      "STSBenchmark\n",
      "Spearman:80.26\n",
      "SICKRelatedness\n",
      "Spearman:68.76\n",
      "Avg:78.31\n",
      "**************************************************\n",
      "\n",
      "Model:output/l2_mono/L2-no-aug-T-princeton-nlp-unsup-simcse-roberta-large-S-microsoft-Multilingual-MiniLM-L12-H384\n",
      "STS12\n",
      "Spearman:71.20\n",
      "STS13\n",
      "Spearman:82.22\n",
      "STS14\n",
      "Spearman:74.14\n",
      "STS15\n",
      "Spearman:82.58\n",
      "STS16\n",
      "Spearman:75.60\n",
      "STSBenchmark\n",
      "Spearman:77.70\n",
      "SICKRelatedness\n",
      "Spearman:69.61\n",
      "Avg:76.15\n",
      "**************************************************\n",
      "\n",
      "Model:output/l2_mono/L2-no-aug-T-princeton-nlp-unsup-simcse-roberta-large-S-distilbert-base-multilingual-cased\n",
      "STS12\n",
      "Spearman:71.56\n",
      "STS13\n",
      "Spearman:82.05\n",
      "STS14\n",
      "Spearman:73.92\n",
      "STS15\n",
      "Spearman:82.85\n",
      "STS16\n",
      "Spearman:76.30\n",
      "STSBenchmark\n",
      "Spearman:77.32\n",
      "SICKRelatedness\n",
      "Spearman:68.73\n",
      "Avg:76.10\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_list = glob(f'output/l2_mono/L2-no-aug-*')\n",
    "for model in model_list:\n",
    "    try:\n",
    "        word_embedding_model = models.Transformer(model, max_seq_length=128)\n",
    "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "        sim_cse = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "    except:\n",
    "        sim_cse = SentenceTransformer(model)\n",
    "    \n",
    "    params = {'task_path': 'SentEval/data/', 'usepytorch': True, 'kfold': 10}\n",
    "    params['encoder'] = sim_cse.encode\n",
    "    se = senteval.engine.SE(params, batcher, prepare)\n",
    "\n",
    "    transfer_tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n",
    "    results = se.eval(transfer_tasks)\n",
    "\n",
    "    print(f\"Model:{model}\")\n",
    "    spearman_val = 0\n",
    "    for key in results.keys():\n",
    "        print(key)\n",
    "    #     print(results)\n",
    "        if key not in ['STSBenchmark','SICKRelatedness']:\n",
    "            result_temp = results[key]['all']['spearman']['all']\n",
    "            spearman_val+=result_temp\n",
    "            print(f\"Spearman:{result_temp*100:.2f}\")\n",
    "        else:\n",
    "            result_temp = results[key]['test']['spearman'].correlation\n",
    "            spearman_val+=result_temp\n",
    "            print(f\"Spearman:{result_temp*100:.2f}\")\n",
    "    print(f\"Avg:{(spearman_val/len(results.keys()))*100:.2f}\")\n",
    "    print(f\"*\"*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKD distill T=Robert-Large (SimCSE-wiki1M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:output/skd/skd_mono-seed-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-MiniLM-L3-H384-uncased\n",
      "STS12\n",
      "Spearman:70.06\n",
      "STS13\n",
      "Spearman:78.67\n",
      "STS14\n",
      "Spearman:69.50\n",
      "STS15\n",
      "Spearman:78.75\n",
      "STS16\n",
      "Spearman:71.73\n",
      "STSBenchmark\n",
      "Spearman:73.45\n",
      "SICKRelatedness\n",
      "Spearman:67.64\n",
      "Avg:72.83\n",
      "**************************************************\n",
      "\n",
      "Model:output/skd/skd_mono-seed-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-TinyBERT_L-6_H-768_v2\n",
      "STS12\n",
      "Spearman:74.65\n",
      "STS13\n",
      "Spearman:82.83\n",
      "STS14\n",
      "Spearman:75.20\n",
      "STS15\n",
      "Spearman:83.24\n",
      "STS16\n",
      "Spearman:78.44\n",
      "STSBenchmark\n",
      "Spearman:78.79\n",
      "SICKRelatedness\n",
      "Spearman:70.78\n",
      "Avg:77.70\n",
      "**************************************************\n",
      "\n",
      "Model:output/skd/skd_mono-seed-T-princeton-nlp-unsup-simcse-roberta-large-S-bert-base-uncased\n",
      "STS12\n",
      "Spearman:75.18\n",
      "STS13\n",
      "Spearman:84.36\n",
      "STS14\n",
      "Spearman:75.77\n",
      "STS15\n",
      "Spearman:82.34\n",
      "STS16\n",
      "Spearman:78.96\n",
      "STSBenchmark\n",
      "Spearman:80.30\n",
      "SICKRelatedness\n",
      "Spearman:71.00\n",
      "Avg:78.27\n",
      "**************************************************\n",
      "\n",
      "Model:output/skd/skd_mono-seed-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-BERT-Small-L-4_H-512_A-8\n",
      "STS12\n",
      "Spearman:71.68\n",
      "STS13\n",
      "Spearman:80.47\n",
      "STS14\n",
      "Spearman:72.42\n",
      "STS15\n",
      "Spearman:80.24\n",
      "STS16\n",
      "Spearman:74.59\n",
      "STSBenchmark\n",
      "Spearman:74.93\n",
      "SICKRelatedness\n",
      "Spearman:68.72\n",
      "Avg:74.72\n",
      "**************************************************\n",
      "\n",
      "Model:output/skd/skd_mono-seed-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-BERT-Tiny_L-2_H-128_A-2\n",
      "STS12\n",
      "Spearman:64.93\n",
      "STS13\n",
      "Spearman:74.67\n",
      "STS14\n",
      "Spearman:64.57\n",
      "STS15\n",
      "Spearman:72.38\n",
      "STS16\n",
      "Spearman:68.22\n",
      "STSBenchmark\n",
      "Spearman:68.53\n",
      "SICKRelatedness\n",
      "Spearman:66.15\n",
      "Avg:68.49\n",
      "**************************************************\n",
      "\n",
      "Model:output/skd/skd_mono-seed-T-princeton-nlp-unsup-simcse-roberta-large-S-roberta-base\n",
      "STS12\n",
      "Spearman:75.15\n",
      "STS13\n",
      "Spearman:82.77\n",
      "STS14\n",
      "Spearman:73.83\n",
      "STS15\n",
      "Spearman:81.83\n",
      "STS16\n",
      "Spearman:78.58\n",
      "STSBenchmark\n",
      "Spearman:78.56\n",
      "SICKRelatedness\n",
      "Spearman:70.76\n",
      "Avg:77.35\n",
      "**************************************************\n",
      "\n",
      "Model:output/skd/skd_mono-seed-T-princeton-nlp-unsup-simcse-roberta-large-S-microsoft-MiniLM-L12-H384-uncased\n",
      "STS12\n",
      "Spearman:70.33\n",
      "STS13\n",
      "Spearman:81.04\n",
      "STS14\n",
      "Spearman:71.24\n",
      "STS15\n",
      "Spearman:80.28\n",
      "STS16\n",
      "Spearman:73.70\n",
      "STSBenchmark\n",
      "Spearman:75.71\n",
      "SICKRelatedness\n",
      "Spearman:69.60\n",
      "Avg:74.56\n",
      "**************************************************\n",
      "\n",
      "Model:output/skd/skd_mono-seed-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-TinyBERT_L-4_H-312_v2\n",
      "STS12\n",
      "Spearman:69.66\n",
      "STS13\n",
      "Spearman:79.03\n",
      "STS14\n",
      "Spearman:70.52\n",
      "STS15\n",
      "Spearman:78.31\n",
      "STS16\n",
      "Spearman:72.13\n",
      "STSBenchmark\n",
      "Spearman:72.69\n",
      "SICKRelatedness\n",
      "Spearman:67.86\n",
      "Avg:72.89\n",
      "**************************************************\n",
      "\n",
      "Model:output/skd/skd_mono-seed-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-BERT-Mini_L-4_H-256_A-4\n",
      "STS12\n",
      "Spearman:69.57\n",
      "STS13\n",
      "Spearman:78.32\n",
      "STS14\n",
      "Spearman:69.50\n",
      "STS15\n",
      "Spearman:77.24\n",
      "STS16\n",
      "Spearman:70.93\n",
      "STSBenchmark\n",
      "Spearman:72.12\n",
      "SICKRelatedness\n",
      "Spearman:66.47\n",
      "Avg:72.02\n",
      "**************************************************\n",
      "\n",
      "Model:output/skd/skd_mono-seed-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-MiniLM-L6-H384-uncased\n",
      "STS12\n",
      "Spearman:69.40\n",
      "STS13\n",
      "Spearman:79.12\n",
      "STS14\n",
      "Spearman:70.87\n",
      "STS15\n",
      "Spearman:78.43\n",
      "STS16\n",
      "Spearman:73.02\n",
      "STSBenchmark\n",
      "Spearman:74.14\n",
      "SICKRelatedness\n",
      "Spearman:68.37\n",
      "Avg:73.34\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_list = glob(f'output/skd/skd_mono-seed-T*')\n",
    "for model in model_list:\n",
    "    sim_cse = SentenceTransformer(model)\n",
    "    \n",
    "    params = {'task_path': 'SentEval/data/', 'usepytorch': True, 'kfold': 10}\n",
    "    params['encoder'] = sim_cse.encode\n",
    "    se = senteval.engine.SE(params, batcher, prepare)\n",
    "\n",
    "    transfer_tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n",
    "    results = se.eval(transfer_tasks)\n",
    "\n",
    "    print(f\"Model:{model}\")\n",
    "    spearman_val = 0\n",
    "    for key in results.keys():\n",
    "        print(key)\n",
    "    #     print(results)\n",
    "        if key not in ['STSBenchmark','SICKRelatedness']:\n",
    "            result_temp = results[key]['all']['spearman']['all']\n",
    "            spearman_val+=result_temp\n",
    "            print(f\"Spearman:{result_temp*100:.2f}\")\n",
    "        else:\n",
    "            result_temp = results[key]['test']['spearman'].correlation\n",
    "            spearman_val+=result_temp\n",
    "            print(f\"Spearman:{result_temp*100:.2f}\")\n",
    "    print(f\"Avg:{(spearman_val/len(results.keys()))*100:.2f}\")\n",
    "    print(f\"*\"*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making distill T=Robert-Large (SimCSE-wiki1M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/m_sentence_embedding/SBERT/SentEval/senteval/sts.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  sent1 = np.array([s.split() for s in sent1])[not_empty_idx]\n",
      "/workspace/m_sentence_embedding/SBERT/SentEval/senteval/sts.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  sent2 = np.array([s.split() for s in sent2])[not_empty_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:output/making/making_mono-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-BERT-Small-L-4_H-512_A-8\n",
      "STS12\n",
      "Spearman:71.05\n",
      "STS13\n",
      "Spearman:81.65\n",
      "STS14\n",
      "Spearman:74.04\n",
      "STS15\n",
      "Spearman:82.49\n",
      "STS16\n",
      "Spearman:75.66\n",
      "STSBenchmark\n",
      "Spearman:77.85\n",
      "SICKRelatedness\n",
      "Spearman:69.35\n",
      "Avg:76.01\n",
      "**************************************************\n",
      "\n",
      "Model:output/making/making_mono-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-MiniLM-L6-H384-uncased\n",
      "STS12\n",
      "Spearman:68.08\n",
      "STS13\n",
      "Spearman:81.22\n",
      "STS14\n",
      "Spearman:72.57\n",
      "STS15\n",
      "Spearman:81.13\n",
      "STS16\n",
      "Spearman:75.14\n",
      "STSBenchmark\n",
      "Spearman:76.11\n",
      "SICKRelatedness\n",
      "Spearman:68.93\n",
      "Avg:74.74\n",
      "**************************************************\n",
      "\n",
      "Model:output/making/making_mono-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-BERT-Mini_L-4_H-256_A-4\n",
      "STS12\n",
      "Spearman:68.12\n",
      "STS13\n",
      "Spearman:78.27\n",
      "STS14\n",
      "Spearman:69.43\n",
      "STS15\n",
      "Spearman:78.94\n",
      "STS16\n",
      "Spearman:71.76\n",
      "STSBenchmark\n",
      "Spearman:73.39\n",
      "SICKRelatedness\n",
      "Spearman:67.44\n",
      "Avg:72.48\n",
      "**************************************************\n",
      "\n",
      "Model:output/making/making_mono-T-princeton-nlp-unsup-simcse-roberta-large-S-bert-base-uncased\n",
      "STS12\n",
      "Spearman:72.82\n",
      "STS13\n",
      "Spearman:84.72\n",
      "STS14\n",
      "Spearman:75.96\n",
      "STS15\n",
      "Spearman:83.35\n",
      "STS16\n",
      "Spearman:78.50\n",
      "STSBenchmark\n",
      "Spearman:80.52\n",
      "SICKRelatedness\n",
      "Spearman:70.59\n",
      "Avg:78.07\n",
      "**************************************************\n",
      "\n",
      "Model:output/making/making_mono-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-MiniLM-L3-H384-uncased\n",
      "STS12\n",
      "Spearman:67.87\n",
      "STS13\n",
      "Spearman:79.72\n",
      "STS14\n",
      "Spearman:71.57\n",
      "STS15\n",
      "Spearman:81.03\n",
      "STS16\n",
      "Spearman:73.67\n",
      "STSBenchmark\n",
      "Spearman:74.37\n",
      "SICKRelatedness\n",
      "Spearman:66.63\n",
      "Avg:73.55\n",
      "**************************************************\n",
      "\n",
      "Model:output/making/making_mono-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-TinyBERT_L-4_H-312_v2\n",
      "STS12\n",
      "Spearman:70.30\n",
      "STS13\n",
      "Spearman:80.43\n",
      "STS14\n",
      "Spearman:72.76\n",
      "STS15\n",
      "Spearman:81.48\n",
      "STS16\n",
      "Spearman:75.40\n",
      "STSBenchmark\n",
      "Spearman:76.30\n",
      "SICKRelatedness\n",
      "Spearman:69.06\n",
      "Avg:75.11\n",
      "**************************************************\n",
      "\n",
      "Model:output/making/making_mono-T-princeton-nlp-unsup-simcse-roberta-large-S-microsoft-MiniLM-L12-H384-uncased\n",
      "STS12\n",
      "Spearman:68.02\n",
      "STS13\n",
      "Spearman:81.57\n",
      "STS14\n",
      "Spearman:72.29\n",
      "STS15\n",
      "Spearman:80.60\n",
      "STS16\n",
      "Spearman:75.34\n",
      "STSBenchmark\n",
      "Spearman:76.80\n",
      "SICKRelatedness\n",
      "Spearman:69.32\n",
      "Avg:74.85\n",
      "**************************************************\n",
      "\n",
      "Model:output/making/making_mono-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-BERT-Tiny_L-2_H-128_A-2\n",
      "STS12\n",
      "Spearman:66.66\n",
      "STS13\n",
      "Spearman:72.39\n",
      "STS14\n",
      "Spearman:63.42\n",
      "STS15\n",
      "Spearman:73.27\n",
      "STS16\n",
      "Spearman:66.23\n",
      "STSBenchmark\n",
      "Spearman:66.55\n",
      "SICKRelatedness\n",
      "Spearman:64.28\n",
      "Avg:67.54\n",
      "**************************************************\n",
      "\n",
      "Model:output/making/making_mono-T-princeton-nlp-unsup-simcse-roberta-large-S-distilbert-base-cased\n",
      "STS12\n",
      "Spearman:73.51\n",
      "STS13\n",
      "Spearman:83.93\n",
      "STS14\n",
      "Spearman:75.98\n",
      "STS15\n",
      "Spearman:83.26\n",
      "STS16\n",
      "Spearman:78.27\n",
      "STSBenchmark\n",
      "Spearman:79.87\n",
      "SICKRelatedness\n",
      "Spearman:69.84\n",
      "Avg:77.81\n",
      "**************************************************\n",
      "\n",
      "Model:output/making/making_mono-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-TinyBERT_L-6_H-768_v2\n",
      "STS12\n",
      "Spearman:73.83\n",
      "STS13\n",
      "Spearman:84.97\n",
      "STS14\n",
      "Spearman:76.81\n",
      "STS15\n",
      "Spearman:84.43\n",
      "STS16\n",
      "Spearman:79.27\n",
      "STSBenchmark\n",
      "Spearman:80.76\n",
      "SICKRelatedness\n",
      "Spearman:70.62\n",
      "Avg:78.67\n",
      "**************************************************\n",
      "\n",
      "Model:output/making/making_mono-T-princeton-nlp-unsup-simcse-roberta-large-S-nreimers-albert-small-v2\n",
      "STS12\n",
      "Spearman:71.49\n",
      "STS13\n",
      "Spearman:82.21\n",
      "STS14\n",
      "Spearman:75.60\n",
      "STS15\n",
      "Spearman:84.12\n",
      "STS16\n",
      "Spearman:77.67\n",
      "STSBenchmark\n",
      "Spearman:79.29\n",
      "SICKRelatedness\n",
      "Spearman:69.68\n",
      "Avg:77.15\n",
      "**************************************************\n",
      "\n",
      "Model:output/making/making_mono-T-princeton-nlp-unsup-simcse-roberta-large-S-roberta-base\n",
      "STS12\n",
      "Spearman:72.29\n",
      "STS13\n",
      "Spearman:84.72\n",
      "STS14\n",
      "Spearman:75.58\n",
      "STS15\n",
      "Spearman:84.39\n",
      "STS16\n",
      "Spearman:80.30\n",
      "STSBenchmark\n",
      "Spearman:80.99\n",
      "SICKRelatedness\n",
      "Spearman:70.55\n",
      "Avg:78.40\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_list = glob(f'output/making/making_mono-T*')\n",
    "\n",
    "for model in model_list:\n",
    "    sim_cse = SentenceTransformer(model)\n",
    "    \n",
    "    params = {'task_path': 'SentEval/data/', 'usepytorch': True, 'kfold': 10}\n",
    "    params['encoder'] = sim_cse.encode\n",
    "    se = senteval.engine.SE(params, batcher, prepare)\n",
    "\n",
    "    transfer_tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n",
    "    results = se.eval(transfer_tasks)\n",
    "\n",
    "    print(f\"Model:{model}\")\n",
    "    spearman_val = 0\n",
    "    for key in results.keys():\n",
    "        print(key)\n",
    "    #     print(results)\n",
    "        if key not in ['STSBenchmark','SICKRelatedness']:\n",
    "            result_temp = results[key]['all']['spearman']['all']\n",
    "            spearman_val+=result_temp\n",
    "            print(f\"Spearman:{result_temp*100:.2f}\")\n",
    "        else:\n",
    "            result_temp = results[key]['test']['spearman'].correlation\n",
    "            spearman_val+=result_temp\n",
    "            print(f\"Spearman:{result_temp*100:.2f}\")\n",
    "    print(f\"Avg:{(spearman_val/len(results.keys()))*100:.2f}\")\n",
    "    print(f\"*\"*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BSL original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:output/bsl_original/seed-BSL_unsup_tuning-nreimers-BERT-Small-L-4_H-512_A-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/m_sentence_embedding/SimCSE/SentEval/senteval/sts.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  sent1 = np.array([s.split() for s in sent1])[not_empty_idx]\n",
      "/workspace/m_sentence_embedding/SimCSE/SentEval/senteval/sts.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  sent2 = np.array([s.split() for s in sent2])[not_empty_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS12\n",
      "Spearman:70.17\n",
      "STS13\n",
      "Spearman:65.73\n",
      "STS14\n",
      "Spearman:66.87\n",
      "STS15\n",
      "Spearman:80.27\n",
      "STS16\n",
      "Spearman:74.53\n",
      "STSBenchmark\n",
      "Spearman:74.26\n",
      "SICKRelatedness\n",
      "Spearman:65.45\n",
      "Avg:71.04\n",
      "**************************************************\n",
      "\n",
      "Model:output/bsl_original/seed-BSL_unsup_tuning-nreimers-TinyBERT_L-6_H-768_v2\n",
      "STS12\n",
      "Spearman:70.23\n",
      "STS13\n",
      "Spearman:67.07\n",
      "STS14\n",
      "Spearman:66.61\n",
      "STS15\n",
      "Spearman:80.43\n",
      "STS16\n",
      "Spearman:75.29\n",
      "STSBenchmark\n",
      "Spearman:75.53\n",
      "SICKRelatedness\n",
      "Spearman:68.81\n",
      "Avg:72.00\n",
      "**************************************************\n",
      "\n",
      "Model:output/bsl_original/seed-BSL_unsup_tuning-nreimers-TinyBERT_L-4_H-312_v2\n",
      "STS12\n",
      "Spearman:67.76\n",
      "STS13\n",
      "Spearman:58.93\n",
      "STS14\n",
      "Spearman:60.21\n",
      "STS15\n",
      "Spearman:76.78\n",
      "STS16\n",
      "Spearman:69.30\n",
      "STSBenchmark\n",
      "Spearman:69.71\n",
      "SICKRelatedness\n",
      "Spearman:66.44\n",
      "Avg:67.02\n",
      "**************************************************\n",
      "\n",
      "Model:output/bsl_original/seed-BSL_unsup_tuning-microsoft-MiniLM-L12-H384-uncased\n",
      "STS12\n",
      "Spearman:63.16\n",
      "STS13\n",
      "Spearman:58.91\n",
      "STS14\n",
      "Spearman:54.38\n",
      "STS15\n",
      "Spearman:72.32\n",
      "STS16\n",
      "Spearman:66.82\n",
      "STSBenchmark\n",
      "Spearman:67.47\n",
      "SICKRelatedness\n",
      "Spearman:64.22\n",
      "Avg:63.90\n",
      "**************************************************\n",
      "\n",
      "Model:output/bsl_original/seed-BSL_unsup_tuning-nreimers-BERT-Tiny_L-2_H-128_A-2\n",
      "STS12\n",
      "Spearman:65.33\n",
      "STS13\n",
      "Spearman:67.05\n",
      "STS14\n",
      "Spearman:62.31\n",
      "STS15\n",
      "Spearman:75.76\n",
      "STS16\n",
      "Spearman:69.51\n",
      "STSBenchmark\n",
      "Spearman:67.22\n",
      "SICKRelatedness\n",
      "Spearman:62.65\n",
      "Avg:67.12\n",
      "**************************************************\n",
      "\n",
      "Model:output/bsl_original/seed-BSL_unsup_tuning-nreimers-BERT-Mini_L-4_H-256_A-4\n",
      "STS12\n",
      "Spearman:68.89\n",
      "STS13\n",
      "Spearman:63.76\n",
      "STS14\n",
      "Spearman:63.57\n",
      "STS15\n",
      "Spearman:78.63\n",
      "STS16\n",
      "Spearman:72.51\n",
      "STSBenchmark\n",
      "Spearman:71.08\n",
      "SICKRelatedness\n",
      "Spearman:64.92\n",
      "Avg:69.05\n",
      "**************************************************\n",
      "\n",
      "Model:output/bsl_original/seed-BSL_unsup_tuning-roberta-base\n",
      "STS12\n",
      "Spearman:72.92\n",
      "STS13\n",
      "Spearman:71.13\n",
      "STS14\n",
      "Spearman:70.74\n",
      "STS15\n",
      "Spearman:78.48\n",
      "STS16\n",
      "Spearman:74.44\n",
      "STSBenchmark\n",
      "Spearman:77.38\n",
      "SICKRelatedness\n",
      "Spearman:68.30\n",
      "Avg:73.34\n",
      "**************************************************\n",
      "\n",
      "Model:output/bsl_original/seed-BSL_unsup_tuning-nreimers-MiniLM-L3-H384-uncased\n",
      "STS12\n",
      "Spearman:69.03\n",
      "STS13\n",
      "Spearman:55.53\n",
      "STS14\n",
      "Spearman:58.13\n",
      "STS15\n",
      "Spearman:74.15\n",
      "STS16\n",
      "Spearman:65.81\n",
      "STSBenchmark\n",
      "Spearman:67.83\n",
      "SICKRelatedness\n",
      "Spearman:63.33\n",
      "Avg:64.83\n",
      "**************************************************\n",
      "\n",
      "Model:output/bsl_original/seed-BSL_unsup_tuning-nreimers-MiniLM-L6-H384-uncased\n",
      "STS12\n",
      "Spearman:64.60\n",
      "STS13\n",
      "Spearman:57.92\n",
      "STS14\n",
      "Spearman:55.45\n",
      "STS15\n",
      "Spearman:73.82\n",
      "STS16\n",
      "Spearman:64.93\n",
      "STSBenchmark\n",
      "Spearman:68.40\n",
      "SICKRelatedness\n",
      "Spearman:63.37\n",
      "Avg:64.07\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pool_mode = 'cls'\n",
    "mlp_mode = True\n",
    "model_list = glob(f'output/bsl_original/seed-BSL_unsup_tuning-*')\n",
    "for model in model_list:\n",
    "    sim_cse = SentenceTransformer(model)\n",
    "    print(f\"Model:{model}\")\n",
    "#     word_embedding_model = models.Transformer(model, max_seq_length=32)\n",
    "#     pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),pooling_mode=pool_mode)\n",
    "#     sim_cse = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "    \n",
    "    params = {'task_path': 'SentEval/data/', 'usepytorch': True, 'kfold': 10}\n",
    "    params['encoder'] = sim_cse.encode\n",
    "    se = senteval.engine.SE(params, batcher, prepare)\n",
    "\n",
    "    transfer_tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n",
    "    results = se.eval(transfer_tasks)\n",
    "\n",
    "    \n",
    "    spearman_val = 0\n",
    "    for key in results.keys():\n",
    "        print(key)\n",
    "    #     print(results)\n",
    "        if key not in ['STSBenchmark','SICKRelatedness']:\n",
    "            result_temp = results[key]['all']['spearman']['all']\n",
    "            spearman_val+=result_temp\n",
    "            print(f\"Spearman:{result_temp*100:.2f}\")\n",
    "        else:\n",
    "            result_temp = results[key]['test']['spearman'].correlation\n",
    "            spearman_val+=result_temp\n",
    "            print(f\"Spearman:{result_temp*100:.2f}\")\n",
    "    print(f\"Avg:{(spearman_val/len(results.keys()))*100:.2f}\")\n",
    "    print(f\"*\"*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimCSE (Supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:output/simcse_original/Simcse_supervised_seed_nreimers_BERT_Tiny_L_2_H_128_A_2_cls_MLPTrue\n",
      "STS12\n",
      "Spearman:67.76\n",
      "STS13\n",
      "Spearman:73.88\n",
      "STS14\n",
      "Spearman:67.58\n",
      "STS15\n",
      "Spearman:77.23\n",
      "STS16\n",
      "Spearman:72.29\n",
      "STSBenchmark\n",
      "Spearman:73.80\n",
      "SICKRelatedness\n",
      "Spearman:73.91\n",
      "Avg:72.35\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_supervised_seed_nreimers_BERT_Mini_L_4_H_256_A_4_cls_MLPTrue\n",
      "STS12\n",
      "Spearman:72.68\n",
      "STS13\n",
      "Spearman:76.34\n",
      "STS14\n",
      "Spearman:73.01\n",
      "STS15\n",
      "Spearman:81.37\n",
      "STS16\n",
      "Spearman:76.58\n",
      "STSBenchmark\n",
      "Spearman:78.33\n",
      "SICKRelatedness\n",
      "Spearman:77.34\n",
      "Avg:76.52\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_supervised_seed_nreimers_BERT_Small_L_4_H_512_A_8_cls_MLPTrue\n",
      "STS12\n",
      "Spearman:73.51\n",
      "STS13\n",
      "Spearman:78.70\n",
      "STS14\n",
      "Spearman:76.51\n",
      "STS15\n",
      "Spearman:83.10\n",
      "STS16\n",
      "Spearman:78.65\n",
      "STSBenchmark\n",
      "Spearman:81.29\n",
      "SICKRelatedness\n",
      "Spearman:78.37\n",
      "Avg:78.59\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_supervised_seed_nreimers_TinyBERT_L_6_H_768_v2_cls_MLPTrue\n",
      "STS12\n",
      "Spearman:76.10\n",
      "STS13\n",
      "Spearman:83.17\n",
      "STS14\n",
      "Spearman:80.06\n",
      "STS15\n",
      "Spearman:85.17\n",
      "STS16\n",
      "Spearman:80.91\n",
      "STSBenchmark\n",
      "Spearman:83.50\n",
      "SICKRelatedness\n",
      "Spearman:79.71\n",
      "Avg:81.23\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_supervised_seed_nreimers_TinyBERT_L_4_H_312_v2_cls_MLPTrue\n",
      "STS12\n",
      "Spearman:74.38\n",
      "STS13\n",
      "Spearman:79.01\n",
      "STS14\n",
      "Spearman:75.73\n",
      "STS15\n",
      "Spearman:82.23\n",
      "STS16\n",
      "Spearman:77.91\n",
      "STSBenchmark\n",
      "Spearman:79.35\n",
      "SICKRelatedness\n",
      "Spearman:78.69\n",
      "Avg:78.19\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_supervised_seed_nreimers_MiniLM_L3_H384_uncased_cls_MLPTrue\n",
      "STS12\n",
      "Spearman:71.04\n",
      "STS13\n",
      "Spearman:77.83\n",
      "STS14\n",
      "Spearman:74.11\n",
      "STS15\n",
      "Spearman:81.40\n",
      "STS16\n",
      "Spearman:77.05\n",
      "STSBenchmark\n",
      "Spearman:77.76\n",
      "SICKRelatedness\n",
      "Spearman:76.21\n",
      "Avg:76.49\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_supervised_seed_microsoft_MiniLM_L12_H384_uncased_cls_MLPTrue\n",
      "STS12\n",
      "Spearman:74.29\n",
      "STS13\n",
      "Spearman:83.07\n",
      "STS14\n",
      "Spearman:79.17\n",
      "STS15\n",
      "Spearman:83.53\n",
      "STS16\n",
      "Spearman:80.40\n",
      "STSBenchmark\n",
      "Spearman:83.14\n",
      "SICKRelatedness\n",
      "Spearman:79.73\n",
      "Avg:80.48\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_supervised_seed_nreimers_MiniLM_L6_H384_uncased_cls_MLPTrue\n",
      "STS12\n",
      "Spearman:73.72\n",
      "STS13\n",
      "Spearman:80.23\n",
      "STS14\n",
      "Spearman:76.66\n",
      "STS15\n",
      "Spearman:81.33\n",
      "STS16\n",
      "Spearman:79.17\n",
      "STSBenchmark\n",
      "Spearman:80.68\n",
      "SICKRelatedness\n",
      "Spearman:78.97\n",
      "Avg:78.68\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pool_mode = 'cls'\n",
    "mlp_mode = True\n",
    "model_list = glob(f'output/simcse_original/Simcse_supervised_seed_*')\n",
    "for model in model_list:\n",
    "    sim_cse = SentenceTransformer(model)\n",
    "    print(f\"Model:{model}\")\n",
    "#     word_embedding_model = models.Transformer(model, max_seq_length=32)\n",
    "#     pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),pooling_mode=pool_mode)\n",
    "#     sim_cse = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "    \n",
    "    params = {'task_path': 'SentEval/data/', 'usepytorch': True, 'kfold': 10}\n",
    "    params['encoder'] = sim_cse.encode\n",
    "    se = senteval.engine.SE(params, batcher, prepare)\n",
    "\n",
    "    transfer_tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n",
    "    results = se.eval(transfer_tasks)\n",
    "\n",
    "    \n",
    "    spearman_val = 0\n",
    "    for key in results.keys():\n",
    "        print(key)\n",
    "    #     print(results)\n",
    "        if key not in ['STSBenchmark','SICKRelatedness']:\n",
    "            result_temp = results[key]['all']['spearman']['all']\n",
    "            spearman_val+=result_temp\n",
    "            print(f\"Spearman:{result_temp*100:.2f}\")\n",
    "        else:\n",
    "            result_temp = results[key]['test']['spearman'].correlation\n",
    "            spearman_val+=result_temp\n",
    "            print(f\"Spearman:{result_temp*100:.2f}\")\n",
    "    print(f\"Avg:{(spearman_val/len(results.keys()))*100:.2f}\")\n",
    "    print(f\"*\"*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimCSE (Wiki1M) Without MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/m_sentence_embedding/SimCSE/SentEval/senteval/sts.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  sent1 = np.array([s.split() for s in sent1])[not_empty_idx]\n",
      "/workspace/m_sentence_embedding/SimCSE/SentEval/senteval/sts.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  sent2 = np.array([s.split() for s in sent2])[not_empty_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:output/simcse_original/Simcse_original_seed_2_nreimers_MiniLM_L6_H384_uncased_cls_MLPTrue_wikiTrue_nliFalse\n",
      "STS12\n",
      "Spearman:46.08\n",
      "STS13\n",
      "Spearman:58.48\n",
      "STS14\n",
      "Spearman:44.47\n",
      "STS15\n",
      "Spearman:57.90\n",
      "STS16\n",
      "Spearman:61.82\n",
      "STSBenchmark\n",
      "Spearman:50.80\n",
      "SICKRelatedness\n",
      "Spearman:55.75\n",
      "Avg:53.61\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_original_seed_2_nreimers_BERT_Tiny_L_2_H_128_A_2_cls_MLPTrue_wikiTrue_nliFalse\n",
      "STS12\n",
      "Spearman:60.97\n",
      "STS13\n",
      "Spearman:69.92\n",
      "STS14\n",
      "Spearman:59.90\n",
      "STS15\n",
      "Spearman:71.39\n",
      "STS16\n",
      "Spearman:66.23\n",
      "STSBenchmark\n",
      "Spearman:60.22\n",
      "SICKRelatedness\n",
      "Spearman:60.06\n",
      "Avg:64.10\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_original_seed_2_nreimers_TinyBERT_L_4_H_312_v2_cls_MLPTrue_wikiTrue_nliFalse\n",
      "STS12\n",
      "Spearman:60.31\n",
      "STS13\n",
      "Spearman:66.74\n",
      "STS14\n",
      "Spearman:57.42\n",
      "STS15\n",
      "Spearman:67.97\n",
      "STS16\n",
      "Spearman:66.43\n",
      "STSBenchmark\n",
      "Spearman:62.57\n",
      "SICKRelatedness\n",
      "Spearman:64.05\n",
      "Avg:63.64\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_original_seed_nreimers_BERT_Mini_L_4_H_256_A_4_cls_MLPTrue_wikiTrue_nliFalse\n",
      "STS12\n",
      "Spearman:59.38\n",
      "STS13\n",
      "Spearman:65.53\n",
      "STS14\n",
      "Spearman:59.11\n",
      "STS15\n",
      "Spearman:72.67\n",
      "STS16\n",
      "Spearman:69.63\n",
      "STSBenchmark\n",
      "Spearman:61.02\n",
      "SICKRelatedness\n",
      "Spearman:62.20\n",
      "Avg:64.22\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_original_seed_2_microsoft_MiniLM_L12_H384_uncased_cls_MLPTrue_wikiTrue_nliFalse\n",
      "STS12\n",
      "Spearman:49.96\n",
      "STS13\n",
      "Spearman:61.15\n",
      "STS14\n",
      "Spearman:50.86\n",
      "STS15\n",
      "Spearman:63.90\n",
      "STS16\n",
      "Spearman:63.99\n",
      "STSBenchmark\n",
      "Spearman:58.18\n",
      "SICKRelatedness\n",
      "Spearman:57.41\n",
      "Avg:57.92\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_original_seed_nreimers_TinyBERT_L_6_H_768_v2_cls_MLPTrue_wikiTrue_nliFalse\n",
      "STS12\n",
      "Spearman:60.72\n",
      "STS13\n",
      "Spearman:67.71\n",
      "STS14\n",
      "Spearman:64.35\n",
      "STS15\n",
      "Spearman:72.94\n",
      "STS16\n",
      "Spearman:69.56\n",
      "STSBenchmark\n",
      "Spearman:67.52\n",
      "SICKRelatedness\n",
      "Spearman:66.61\n",
      "Avg:67.06\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_original_seed_nreimers_MiniLM_L6_H384_uncased_cls_MLPTrue_wikiTrue_nliFalse\n",
      "STS12\n",
      "Spearman:49.40\n",
      "STS13\n",
      "Spearman:59.14\n",
      "STS14\n",
      "Spearman:45.43\n",
      "STS15\n",
      "Spearman:59.28\n",
      "STS16\n",
      "Spearman:61.86\n",
      "STSBenchmark\n",
      "Spearman:51.82\n",
      "SICKRelatedness\n",
      "Spearman:56.07\n",
      "Avg:54.71\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_original_seed_2_nreimers_BERT_Small_L_4_H_512_A_8_cls_MLPTrue_wikiTrue_nliFalse\n",
      "STS12\n",
      "Spearman:57.09\n",
      "STS13\n",
      "Spearman:66.03\n",
      "STS14\n",
      "Spearman:59.95\n",
      "STS15\n",
      "Spearman:72.64\n",
      "STS16\n",
      "Spearman:69.88\n",
      "STSBenchmark\n",
      "Spearman:61.87\n",
      "SICKRelatedness\n",
      "Spearman:63.98\n",
      "Avg:64.49\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_original_seed_2_nreimers_BERT_Mini_L_4_H_256_A_4_cls_MLPTrue_wikiTrue_nliFalse\n",
      "STS12\n",
      "Spearman:57.55\n",
      "STS13\n",
      "Spearman:66.59\n",
      "STS14\n",
      "Spearman:59.47\n",
      "STS15\n",
      "Spearman:72.53\n",
      "STS16\n",
      "Spearman:69.90\n",
      "STSBenchmark\n",
      "Spearman:60.79\n",
      "SICKRelatedness\n",
      "Spearman:61.71\n",
      "Avg:64.08\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_original_seed_nreimers_MiniLM_L3_H384_uncased_cls_MLPTrue_wikiTrue_nliFalse\n",
      "STS12\n",
      "Spearman:29.78\n",
      "STS13\n",
      "Spearman:37.48\n",
      "STS14\n",
      "Spearman:26.68\n",
      "STS15\n",
      "Spearman:45.82\n",
      "STS16\n",
      "Spearman:48.98\n",
      "STSBenchmark\n",
      "Spearman:35.63\n",
      "SICKRelatedness\n",
      "Spearman:44.28\n",
      "Avg:38.38\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_original_seed_nreimers_BERT_Small_L_4_H_512_A_8_cls_MLPTrue_wikiTrue_nliFalse\n",
      "STS12\n",
      "Spearman:55.75\n",
      "STS13\n",
      "Spearman:66.86\n",
      "STS14\n",
      "Spearman:60.07\n",
      "STS15\n",
      "Spearman:72.17\n",
      "STS16\n",
      "Spearman:69.25\n",
      "STSBenchmark\n",
      "Spearman:60.54\n",
      "SICKRelatedness\n",
      "Spearman:63.18\n",
      "Avg:63.97\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_original_seed_2_nreimers_MiniLM_L3_H384_uncased_cls_MLPTrue_wikiTrue_nliFalse\n",
      "STS12\n",
      "Spearman:28.78\n",
      "STS13\n",
      "Spearman:38.05\n",
      "STS14\n",
      "Spearman:33.82\n",
      "STS15\n",
      "Spearman:50.57\n",
      "STS16\n",
      "Spearman:46.64\n",
      "STSBenchmark\n",
      "Spearman:31.47\n",
      "SICKRelatedness\n",
      "Spearman:48.41\n",
      "Avg:39.68\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_original_seed_nreimers_BERT_Tiny_L_2_H_128_A_2_cls_MLPTrue_wikiTrue_nliFalse\n",
      "STS12\n",
      "Spearman:60.71\n",
      "STS13\n",
      "Spearman:70.44\n",
      "STS14\n",
      "Spearman:60.21\n",
      "STS15\n",
      "Spearman:71.51\n",
      "STS16\n",
      "Spearman:66.86\n",
      "STSBenchmark\n",
      "Spearman:60.33\n",
      "SICKRelatedness\n",
      "Spearman:59.38\n",
      "Avg:64.21\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_original_seed_microsoft_MiniLM_L12_H384_uncased_cls_MLPTrue_wikiTrue_nliFalse\n",
      "STS12\n",
      "Spearman:49.63\n",
      "STS13\n",
      "Spearman:61.57\n",
      "STS14\n",
      "Spearman:50.33\n",
      "STS15\n",
      "Spearman:64.98\n",
      "STS16\n",
      "Spearman:64.55\n",
      "STSBenchmark\n",
      "Spearman:59.64\n",
      "SICKRelatedness\n",
      "Spearman:57.57\n",
      "Avg:58.33\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_original_seed_2_nreimers_TinyBERT_L_6_H_768_v2_cls_MLPTrue_wikiTrue_nliFalse\n",
      "STS12\n",
      "Spearman:57.69\n",
      "STS13\n",
      "Spearman:71.31\n",
      "STS14\n",
      "Spearman:63.22\n",
      "STS15\n",
      "Spearman:72.72\n",
      "STS16\n",
      "Spearman:67.89\n",
      "STSBenchmark\n",
      "Spearman:65.09\n",
      "SICKRelatedness\n",
      "Spearman:67.02\n",
      "Avg:66.42\n",
      "**************************************************\n",
      "\n",
      "Model:output/simcse_original/Simcse_original_seed_nreimers_TinyBERT_L_4_H_312_v2_cls_MLPTrue_wikiTrue_nliFalse\n",
      "STS12\n",
      "Spearman:60.11\n",
      "STS13\n",
      "Spearman:67.39\n",
      "STS14\n",
      "Spearman:58.32\n",
      "STS15\n",
      "Spearman:69.46\n",
      "STS16\n",
      "Spearman:67.29\n",
      "STSBenchmark\n",
      "Spearman:63.23\n",
      "SICKRelatedness\n",
      "Spearman:65.49\n",
      "Avg:64.47\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_list = glob(f'output/simcse_original/Simcse_original_seed_*wikiTrue_nliFalse')\n",
    "# model_list = ['output/Simcse_original_bert_base_uncased_cls_MLPTrue']\n",
    "# model_list = glob(f'output/Simcse_original_*large*{pool_mode}*MLP{mlp_mode}*')\n",
    "for model in model_list:\n",
    "#     sim_cse = SentenceTransformer(model)\n",
    "    word_embedding_model = models.Transformer(model, max_seq_length=32)\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),pooling_mode=pool_mode)\n",
    "    sim_cse = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "    params = {'task_path': 'SentEval/data/', 'usepytorch': True, 'kfold': 10}\n",
    "    params['encoder'] = sim_cse.encode\n",
    "    se = senteval.engine.SE(params, batcher, prepare)\n",
    "\n",
    "    transfer_tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n",
    "    results = se.eval(transfer_tasks)\n",
    "\n",
    "    print(f\"Model:{model}\")\n",
    "    spearman_val = 0\n",
    "    for key in results.keys():\n",
    "        print(key)\n",
    "    #     print(results)\n",
    "        if key not in ['STSBenchmark','SICKRelatedness']:\n",
    "            result_temp = results[key]['all']['spearman']['all']\n",
    "            spearman_val+=result_temp\n",
    "            print(f\"Spearman:{result_temp*100:.2f}\")\n",
    "        else:\n",
    "            result_temp = results[key]['test']['spearman'].correlation\n",
    "            spearman_val+=result_temp\n",
    "            print(f\"Spearman:{result_temp*100:.2f}\")\n",
    "    print(f\"Avg:{(spearman_val/len(results.keys()))*100:.2f}\")\n",
    "    print(f\"*\"*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
